{
  "results": {
    "kobest_boolq": {
      "acc": 0.5049857549857549,
      "acc_stderr": 0.013348103841229718,
      "macro_f1": 0.34185206543843144,
      "macro_f1_stderr": 0.006853236003818263
    },
    "kobest_copa": {
      "acc": 0.596,
      "acc_stderr": 0.01552498067712258,
      "macro_f1": 0.5955858799410596,
      "macro_f1_stderr": 0.015545377379128918
    },
    "kobest_hellaswag": {
      "acc": 0.392,
      "acc_stderr": 0.021854684955611256,
      "acc_norm": 0.51,
      "acc_norm_stderr": 0.022378596989230785,
      "macro_f1": 0.3880623840445269,
      "macro_f1_stderr": 0.02168756322140791
    },
    "kobest_sentineg": {
      "acc": 0.49622166246851385,
      "acc_stderr": 0.025125227983562776,
      "macro_f1": 0.33164983164983164,
      "macro_f1_stderr": 0.011251703709904158
    },
    "korunsmile": {
      "f1": 0.4024160842334494,
      "f1_stderr": 0.00806403325040705
    },
    "pawsx_ko": {
      "acc": 0.577,
      "acc_stderr": 0.011049730687855398
    }
  },
  "versions": {
    "kobest_boolq": 0,
    "kobest_copa": 0,
    "kobest_hellaswag": 0,
    "kobest_sentineg": 0,
    "korunsmile": 0,
    "pawsx_ko": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=meta-llama/Meta-Llama-3-8B-Instruct,use_accelerate=true,trust_remote_code=true,peft=/home/hyohyeongjang/bigdata/ModelTrainer/llm-kr/outputs/checkpoints/meta-llama/Meta-Llama-3-8B-Instruct/lora/checkpoint-4500",
    "num_fewshot": 0,
    "batch_size": "32",
    "device": null,
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {}
  }
}