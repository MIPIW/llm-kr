{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TextStreamer\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "import argparse, logging, os\n",
    "from tqdm import tqdm\n",
    "import torch, random\n",
    "import numpy as np\n",
    "\n",
    "# LoRA modules\n",
    "from peft.mapping import get_peft_model\n",
    "from peft.tuners.lora import LoraConfig\n",
    "from peft.utils.peft_types import TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset_dir):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FormattingFunction:\n",
    "    def __init__(self, model_checkpoint):\n",
    "        self.instruction_all = {\n",
    "            \"meta-llama/Meta-Llama-3-8B-Instruct\": \"\"\"\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        당신의 역할은 한국어로 답변하는 **한국어 AI 어시트턴트**입니다. 주어진 질문에 대해 한국어로 답변해주세요.<|eot_id|>\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        아래 질문을 한국어로 정확하게 답변해주세요. **질문**: {}<|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\\n\\n{}<|eot_id|>\n",
    "        <|end_of_text|>\"\"\",\n",
    "            \"google/gemma-2-9b-it\": \"\"\"\n",
    "        <bos><start_of_turn>model\n",
    "        당신의 역할은 한국어로 답변하는 **한국어 AI 어시트턴트**입니다. 주어진 질문에 대해 한국어로 답변해주세요.<end_of_turn>\n",
    "        <start_of_turn>user\n",
    "        아래 질문을 한국어로 정확하게 답변해주세요. **질문**: {}<end_of_turn>\n",
    "        <start_of_turn>model\n",
    "        {}<end_of_turn>\n",
    "        <eos>\"\"\",\n",
    "            \"Qwen/Qwen2-7B-Instruct\": \"\"\"\n",
    "        <|im_start|>system\n",
    "        당신의 역할은 한국어로 답변하는 **한국어 AI 어시트턴트**입니다. 주어진 질문에 대해 한국어로 답변해주세요.\\n<|im_end|>\n",
    "        <|im_start|>user\n",
    "        아래 질문을 한국어로 정확하게 답변해주세요. **질문**: {}<|im_end|>\n",
    "        <|im_start|>system\n",
    "        {}<|im_end|>\n",
    "        <|endoftext|>\"\"\",\n",
    "        }\n",
    "\n",
    "        self.instruction = self.instruction_all[model_checkpoint]\n",
    "\n",
    "    def __call__(self, examples):\n",
    "\n",
    "        final_texts = []\n",
    "        for i in tqdm(range(len(examples[\"input\"]))):\n",
    "            final_text = self.instruction.format(\n",
    "                examples[\"input\"][i], examples[\"output\"][i]\n",
    "            )\n",
    "            final_texts.append(final_text)\n",
    "\n",
    "        return final_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "\n",
    "\n",
    "# model class\n",
    "class ModelInitiator:\n",
    "    def __init__(self, model_checkpoint):\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "\n",
    "    def __call__(self):\n",
    "        if self.model_checkpoint == \"google/gemma-2-9b-it\":\n",
    "            path = \"../../outputs/checkpoints/google/gemma-2-9b-it/lora/checkpoint-3500\"\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                path,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                attn_implementation=\"eager\",\n",
    "            )\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_checkpoint,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "            )\n",
    "\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)\n",
    "\n",
    "        # padding 처리\n",
    "        # llama3-8b\n",
    "        if \"Llama-3\" in self.model_checkpoint:  # \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "            tokenizer.pad_token = tokenizer.pad_token\n",
    "            tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        else:\n",
    "            pass  # qwen2-it, gemma2-it는 pad 토큰 이미 설정되어 있음\n",
    "            # qwen2: <|endoftext|> # https://huggingface.co/Qwen/Qwen2-7B-Instruct/blob/main/tokenizer_config.json#L36\n",
    "            # gemma2: <pad>  # https://huggingface.co/google/gemma-2b-it/blob/main/tokenizer_config.json#L1511\n",
    "\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        return model, tokenizer  # , self.target_modules[self.model_checkpoint]\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    np.random.default_rng(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # initialize model\n",
    "\n",
    "    # use same tokenizer checkpoint with model\n",
    "    modelInitiator = ModelInitiator(args.model)\n",
    "\n",
    "    model, tokenizer = modelInitiator()\n",
    "    seed_everything(args.seed)\n",
    "\n",
    "    # initialize loraconfig\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config=peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # prepare dataset\n",
    "    # train_dataset = MyDataset(args.train_dataset_dir)\n",
    "    # eval_dataset = MyDataset(args.eval_dataset_dir)\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        delimiter=\",\",\n",
    "        data_files={\n",
    "            \"train\": [args.train_dataset_dir],\n",
    "            \"validation\": [args.eval_dataset_dir],\n",
    "        },\n",
    "        keep_default_na=False,\n",
    "    )\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "\n",
    "    # initialize training arguments\n",
    "    trainingarguments = TrainingArguments(\n",
    "        output_dir=args.output_dir + args.model + \"/lora\",\n",
    "        save_strategy=\"steps\",  # \"epoch\"\n",
    "        eval_strategy=\"steps\",  # \"no\"\n",
    "        save_steps=args.save_steps,\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=1,\n",
    "        num_train_epochs=args.epochs,\n",
    "        logging_steps=args.logging_steps,\n",
    "        fp16=True,\n",
    "        metric_for_best_model=\"loss\",  # evaluation_strategy 설정 시 eval_loss로 평가 # \"train_loss\"\n",
    "        load_best_model_at_end=False,\n",
    "        seed=args.seed,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "    )\n",
    "\n",
    "    # preparing others\n",
    "    formattingfunction = FormattingFunction(\n",
    "        args.model\n",
    "    )  # model마다 다른 instruction template 사용하도록\n",
    "\n",
    "    collator_fn = DataCollatorForLanguageModeling(\n",
    "        tokenizer, mlm=False\n",
    "    )  # mlm=False: Autoregressive\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=trainingarguments,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        formatting_func=formattingfunction,\n",
    "        max_seq_length=2048,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=collator_fn,\n",
    "    )\n",
    "\n",
    "    if args.model == \"google/gemma-2-9b-it\":\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "    else:\n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: ''. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/model_train_py39/lib/python3.9/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.conda/envs/model_train_py39/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/model_train_py39/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:160\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: ''.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m\n\u001b[1;32m     30\u001b[0m     args \u001b[38;5;241m=\u001b[39m argparse\u001b[38;5;241m.\u001b[39mNamespace(\n\u001b[1;32m     31\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel_checkpoint,\n\u001b[1;32m     32\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_checkpoint,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m         eval_dataset_dir\u001b[38;5;241m=\u001b[39meval_dataset_dir,\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m types \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margumentparser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjupyter_inline\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     modelInitiator \u001b[38;5;241m=\u001b[39m ModelInitiator(args\u001b[38;5;241m.\u001b[39mmodel, args\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[0;32m----> 8\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mmodelInitiator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# initialize loraconfig\u001b[39;00m\n\u001b[1;32m     11\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     12\u001b[0m     task_type\u001b[38;5;241m=\u001b[39mTaskType\u001b[38;5;241m.\u001b[39mCAUSAL_LM,\n\u001b[1;32m     13\u001b[0m     inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     17\u001b[0m )\n",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m, in \u001b[0;36mModelInitiator.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_checkpoint)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# padding 처리\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# llama3-8b\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/model_train_py39/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:485\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/model_train_py39/lib/python3.9/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: ''. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "types = \"argumentparser\"\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if types == \"argumentparser\":\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\"--model\", default=None, type=str, required=True)\n",
    "        parser.add_argument(\"--seed\", default=42, type=int, required=False)\n",
    "        parser.add_argument(\n",
    "            \"--output_dir\",\n",
    "            default=\"../../outputs/checkpoints/\",\n",
    "            type=str,\n",
    "            required=False,\n",
    "        )\n",
    "        parser.add_argument(\"--train_batch_size\", default=None, type=int, required=True)\n",
    "        parser.add_argument(\"--eval_batch_size\", default=None, type=int, required=True)\n",
    "        parser.add_argument(\"--epochs\", default=None, type=int, required=True)\n",
    "\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    if types == \"jupyter_inline\":\n",
    "        model_checkpoint = \"\"\n",
    "        tokenizer_checkpoint = \"\"\n",
    "        more_args_value = \"\"\n",
    "        output_dir_value = \"\"\n",
    "        train_batch_size = \"\"\n",
    "        eval_batch_size = \"\"\n",
    "        epochs = \"\"\n",
    "\n",
    "        args = argparse.Namespace(\n",
    "            model=model_checkpoint,\n",
    "            tokenizer=tokenizer_checkpoint,\n",
    "            output_dir=output_dir_value,\n",
    "            train_batch_size=train_batch_size,\n",
    "            eval_batch_size=eval_batch_size,\n",
    "            epochs=epochs,\n",
    "        )\n",
    "\n",
    "    # custom preset\n",
    "    train_dataset_dir = (\n",
    "        \"../../data/oig-smallchip2-dedu-slice_reviewed_week1-7_instruction_train.csv\"\n",
    "    )\n",
    "    eval_dataset_dir = (\n",
    "        \"../../data/oig-smallchip2-dedu-slice_reviewed_week1-7_instruction_valid.csv\"\n",
    "    )\n",
    "    args.train_dataset_dir = train_dataset_dir\n",
    "    args.eval_dataset_dir = eval_dataset_dir\n",
    "    args.save_steps = 500\n",
    "    args.logging_steps = 500\n",
    "\n",
    "    if types in [\"argumentparser\", \"jupyter_inline\"]:\n",
    "        main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
