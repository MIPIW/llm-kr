{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TextStreamer\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import Accelerator\n",
    "import argparse, logging, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LoRA modules\n",
    "from peft.mapping import get_peft_model\n",
    "from peft.tuners.lora import LoraConfig\n",
    "from peft.utils.peft_types import TaskType\n",
    "from peft.peft_model import PeftModelForCausalLM\n",
    "from peft.config import PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.instruction = \"\"\"<|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        당신은 한국어를 정확하게 구사하는 AI 어시트턴트입니다. 질문에 대해 한국어로 답변해주세요<|eot_id|>\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        AI 어시스턴트님, 아래 질문을 한국어로 전문적으로 답변해주세요. 중요한 키워드는 볼드체로 표기하세요.\n",
    "        질문: {}<|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input):\n",
    "        template = self.instruction.format(input)\n",
    "        input_ids = self.tokenizer(template, return_tensors=\"pt\")[\"input_ids\"].to(\n",
    "            self.model.device\n",
    "        )\n",
    "        terminators = [\n",
    "            self.tokenizer.eos_token_id,\n",
    "            self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "        ]\n",
    "\n",
    "        output = self.model.generate(\n",
    "            input_ids,\n",
    "            streamer=TextStreamer(self.tokenizer),\n",
    "            do_sample=True,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.3,\n",
    "            top_p=0.92,\n",
    "            top_k=2,\n",
    "            # no_repeat_ngram_size=3,\n",
    "            renormalize_logits=True,\n",
    "            eos_token_id=terminators,\n",
    "        )\n",
    "\n",
    "        response = output[0][input_ids.shape[-1] :]\n",
    "        response = self.tokenizer.decode(response, skip_special_tokens=True)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class\n",
    "class ModelInitiator:\n",
    "    def __init__(self, model_checkpoint, tokenizer_checkpoint):\n",
    "        \n",
    "        self.model_checkpoint = model_checkpoint\n",
    "        self.tokenizer_checkpoint = tokenizer_checkpoint\n",
    "\n",
    "    def __call__(self):\n",
    "        config = PeftConfig.from_pretrained(self.model_checkpoint)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.base_model_name_or_path\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )\n",
    "        model = PeftModelForCausalLM.from_pretrained(model, self.model_checkpoint)\n",
    "        model.eval()\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_checkpoint)\n",
    "        tokenizer.pad_token = tokenizer.pad_token\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # initialize model\n",
    "\n",
    "    if args.tokenizer == None:\n",
    "        modelInitiator = ModelInitiator(args.model, args.model)\n",
    "    else:\n",
    "        modelInitiator = ModelInitiator(args.model, args.tokenizer)\n",
    "\n",
    "    model, tokenizer = modelInitiator()\n",
    "    generator = Generator(model, tokenizer)\n",
    "\n",
    "    return generator(args.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = \"jupyter_inline\"\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if types == \"argumentparser\":\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\"--model\", default=None, type=str, required=True)\n",
    "        parser.add_argument(\"--tokenizer\", default=None, type=str, required=False)\n",
    "        parser.add_argument(\"--inputs\", default=None, type=str, required=False)\n",
    "\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    if types == \"jupyter_inline\":\n",
    "        model_checkpoint = \"\"\n",
    "        tokenizer_checkpoint = \"\"\n",
    "        inputs = \"\"\n",
    "\n",
    "        args = argparse.Namespace(\n",
    "            model=model_checkpoint, tokenizer=tokenizer_checkpoint, inputs=inputs\n",
    "        )\n",
    "\n",
    "    if types in [\"argumentparser\", \"jupyter_inline\"]:\n",
    "        main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
